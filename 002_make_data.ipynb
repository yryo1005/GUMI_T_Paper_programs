{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ed8961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'Japanese_BPEEncoder_V2'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "\n",
    "import subprocess\n",
    "if not os.path.exists(\"Japanese_BPEEncoder_V2\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/tanreinama/Japanese-BPEEncoder_V2.git\", \"Japanese_BPEEncoder_V2\"])\n",
    "from Japanese_BPEEncoder_V2.encode_swe import SWEEncoder_ja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aad566",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43102bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_WORD_IMAGE_ONLY = True\n",
    "REAL_IMAGE_ONLY = True\n",
    "REAL_IMAGE_THRESHOLD = 0.75\n",
    "NO_UNIQUE_NOUN_SENTENCE_ONLY = True\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "MIN_FREQUENCY = 16\n",
    "\n",
    "data_dir = f\"../datas/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREQUENCY}/\"\n",
    "os.makedirs(data_dir, exist_ok = True)\n",
    "with open(f\"{data_dir}data_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"NO_WORD_IMAGE_ONLY\": NO_WORD_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_ONLY\": REAL_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_THRESHOLD\": REAL_IMAGE_THRESHOLD,\n",
    "        \"NO_UNIQUE_NOUN_SENTENCE_ONLY\": NO_UNIQUE_NOUN_SENTENCE_ONLY,\n",
    "\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MIN_FREQUENCY\": MIN_FREQUENCY,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0a0aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../datas/Bokete_Dataset/boke_data_assemble/\"\n",
    "IMAGE_DIR = \"../datas/Bokete_Dataset/boke_image/\"\n",
    "\n",
    "with open('Japanese_BPEEncoder_V2/ja-swe32kfix.txt') as f:\n",
    "    bpe = f.read().split('\\n')\n",
    "\n",
    "with open('Japanese_BPEEncoder_V2/emoji.json') as f:\n",
    "    emoji = json.loads(f.read())\n",
    "\n",
    "SWE_tokenizer = SWEEncoder_ja(bpe, emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3745540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_contains_invalid_characters(text):\n",
    "    # ひらがな（\\u3040-\\u309F）\n",
    "    # カタカナ（\\u30A0-\\u30FF）\n",
    "    # 漢字（\\u4E00-\\u9FFF）\n",
    "    # 句読点「、。」（直接列挙）\n",
    "    pattern = r\"^[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF、。!?！？ー]*$\"\n",
    "    return not re.fullmatch(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dict()\n",
    "word_count_dict = dict()\n",
    "\n",
    "\n",
    "\n",
    "# ファイル名でソートすることで他のコンピュータでも同様の結果となる\n",
    "for JP in tqdm(sorted(os.listdir(DATA_DIR))):\n",
    "\n",
    "    # 画像があるか\n",
    "    n = JP.split(\".\")[0]\n",
    "    if not os.path.exists(f\"{IMAGE_DIR}{n}.jpg\"):\n",
    "        continue\n",
    "\n",
    "    with open(f\"{DATA_DIR}{JP}\", \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    \n",
    "    image_information = d[\"image_infomation\"]\n",
    "\n",
    "    # 現実の画像であるか\n",
    "    if REAL_IMAGE_ONLY:\n",
    "        if image_information[\"is_photographic_probability\"] < REAL_IMAGE_THRESHOLD:\n",
    "            continue\n",
    "    \n",
    "    # OCRで文字を含む画像であるか\n",
    "    if NO_WORD_IMAGE_ONLY:\n",
    "        if len(image_information[\"ocr\"]) != 0:\n",
    "            continue\n",
    "    \n",
    "    tmp_bokes = list()\n",
    "    for B in d[\"bokes\"]:\n",
    "\n",
    "        # 数字，ローマ字，記号を含む文章であるか\n",
    "        if is_contains_invalid_characters(B[\"boke\"]):\n",
    "            continue\n",
    "        \n",
    "        # 固有名詞を含む文章であるか\n",
    "        if NO_UNIQUE_NOUN_SENTENCE_ONLY:\n",
    "            if len(B[\"unique_nouns\"]) != 0:\n",
    "                continue\n",
    "        \n",
    "        boke = B[\"boke\"].replace(\"�\", \"\")\n",
    "        tokenized_boke = SWE_tokenizer.encode(boke)\n",
    "\n",
    "        # 文章の長さが最大値を超えているか\n",
    "        if not (MIN_SENTENCE_LENGTH <= len(tokenized_boke) <= MAX_SENTENCE_LENGTH):\n",
    "            continue\n",
    "        \n",
    "        tmp_bokes.append( {\n",
    "            \"boke\": boke,\n",
    "            \"tmp_tokenized_boke\": tokenized_boke\n",
    "        } )\n",
    "\n",
    "        for I in tokenized_boke:\n",
    "            try:\n",
    "                word_count_dict[I] += 1\n",
    "            except:\n",
    "                word_count_dict[I] = 1\n",
    "\n",
    "    data_dict[n] = tmp_bokes\n",
    "\n",
    "# 単語の最小出現回数を満たすか\n",
    "tmp_data_dict = dict()\n",
    "words = list()\n",
    "for K, V in data_dict.items():\n",
    "    tmp_bokes = list()\n",
    "    for B in V:\n",
    "\n",
    "\n",
    "        flag = False\n",
    "        for I in B[\"tmp_tokenized_boke\"]:\n",
    "            if word_count_dict[I] < MIN_FREQUENCY:\n",
    "                flag = True\n",
    "                break\n",
    "        if flag:\n",
    "            continue\n",
    "\n",
    "        tmp_bokes.append( B )\n",
    "        words += B[\"tmp_tokenized_boke\"]\n",
    "\n",
    "    if len(tmp_bokes) == 0:\n",
    "        continue\n",
    "    tmp_data_dict[K] = tmp_bokes\n",
    "words = list(set(words))\n",
    "data_dict = tmp_data_dict\n",
    "\n",
    "#\n",
    "index_to_index = {\n",
    "    W: i + 3 for i, W in enumerate(words)\n",
    "}\n",
    "index_to_word = {\n",
    "    i + 3: SWE_tokenizer.decode([W]) for i, W in enumerate(words) if SWE_tokenizer.decode([W]) != \"�\"\n",
    "}\n",
    "index_to_word[0] = \"<PAD>\"\n",
    "index_to_word[1] = \"<BOS>\"\n",
    "index_to_word[2] = \"<EOS>\"\n",
    "\n",
    "tmp_data_dict = dict()\n",
    "for K, V in tqdm(data_dict.items()):\n",
    "    tmp_bokes = list()\n",
    "    for B in V:\n",
    "        tokenized_boke = list()\n",
    "        for I in B[\"tmp_tokenized_boke\"]:\n",
    "            if SWE_tokenizer.decode([I]) == \"�\": continue \n",
    "            \n",
    "            tokenized_boke.append(index_to_index[I])\n",
    "        tmp_bokes.append( {\n",
    "            \"boke\": B[\"boke\"],\n",
    "            \"tokenized_boke\": tokenized_boke\n",
    "        } )\n",
    "    \n",
    "    tmp_data_dict[K] = tmp_bokes\n",
    "\n",
    "data_dict = tmp_data_dict\n",
    "\n",
    "# # tokenizerの定義\n",
    "model = WordLevel(vocab = {V:K for K, V in index_to_word.items()})\n",
    "tokenizer = Tokenizer(model)\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "#\n",
    "len(data_dict), sum([len(V) for V in data_dict.values()]), len(index_to_word), tokenizer.get_vocab()[\"<PAD>\"], tokenizer.get_vocab()[\"<BOS>\"], tokenizer.get_vocab()[\"<EOS>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_numbers, test_image_numbers = train_test_split(sorted(list(data_dict.keys())), \n",
    "                                                           test_size = 0.01,\n",
    "                                                           random_state = 42)\n",
    "\n",
    "# image\n",
    "train_inputs_1 = list()\n",
    "# sentence\n",
    "train_inputs_2 = list()\n",
    "train_teacher_signals = list()\n",
    "for N in tqdm(train_image_numbers):\n",
    "    for B in data_dict[N]:\n",
    "\n",
    "        train_inputs_1.append( int(N) )\n",
    "\n",
    "        tokenized_boke = [1] + B[\"tokenized_boke\"] + [2]\n",
    "        tokenized_boke += [0] * (MAX_SENTENCE_LENGTH + 2 - len(tokenized_boke))\n",
    "        train_inputs_2.append( tokenized_boke[:-1] )\n",
    "        train_teacher_signals.append( tokenized_boke[1:] )\n",
    "train_inputs_1 = np.array(train_inputs_1)\n",
    "train_inputs_2 = np.array(train_inputs_2)\n",
    "train_teacher_signals = np.array(train_teacher_signals)\n",
    "\n",
    "#\n",
    "# image\n",
    "test_inputs_1 = list()\n",
    "# sentence\n",
    "test_inputs_2 = list()\n",
    "test_teacher_signals = list()\n",
    "for N in tqdm(test_image_numbers):\n",
    "    for B in data_dict[N]:\n",
    "\n",
    "        test_inputs_1.append( int(N) )\n",
    "\n",
    "        tokenized_boke = [1] + B[\"tokenized_boke\"] + [2]\n",
    "        tokenized_boke += [0] * (MAX_SENTENCE_LENGTH + 2 - len(tokenized_boke))\n",
    "        test_inputs_2.append( tokenized_boke[:-1] )\n",
    "        test_teacher_signals.append( tokenized_boke[1:] )\n",
    "test_inputs_1 = np.array(test_inputs_1)\n",
    "test_inputs_2 = np.array(test_inputs_2)\n",
    "test_teacher_signals = np.array(test_teacher_signals)\n",
    "\n",
    "#\n",
    "len(train_image_numbers), len(test_image_numbers), train_inputs_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(f\"{data_dir}tokenizer.json\")\n",
    "np.save(f\"{data_dir}train_inputs_1.npy\", train_inputs_1)\n",
    "np.save(f\"{data_dir}train_inputs_2.npy\", train_inputs_2)\n",
    "np.save(f\"{data_dir}train_teacher_signals.npy\", train_teacher_signals)\n",
    "np.save(f\"{data_dir}test_inputs_1.npy\", test_inputs_1)\n",
    "np.save(f\"{data_dir}test_inputs_2.npy\", test_inputs_2)\n",
    "np.save(f\"{data_dir}test_teacher_signals.npy\", test_teacher_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b56c2",
   "metadata": {},
   "source": [
    "# アンケート用の大喜利を選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs_1 = np.load(f\"{data_dir}test_inputs_1.npy\")\n",
    "test_image_numbers = list(set(test_inputs_1.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b1e37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2202/2202 [00:01<00:00, 1187.07it/s]\n"
     ]
    }
   ],
   "source": [
    "test_inputs_1 = np.load(f\"{data_dir}test_inputs_1.npy\")\n",
    "test_image_numbers = list(set(test_inputs_1.tolist()))\n",
    "\n",
    "data_dict = dict()\n",
    "for N in tqdm(test_image_numbers):\n",
    "    with open(f\"{DATA_DIR}{N}.json\", \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    \n",
    "    tmp_bokes = list()\n",
    "    for B in d[\"bokes\"]:\n",
    "        # 数字，ローマ字，記号を含む文章であるか\n",
    "        if is_contains_invalid_characters(B[\"boke\"]):\n",
    "            continue\n",
    "\n",
    "        # 固有名詞を含む文章であるか\n",
    "        if NO_UNIQUE_NOUN_SENTENCE_ONLY:\n",
    "            if len(B[\"unique_nouns\"]) != 0:\n",
    "                continue\n",
    "        boke = B[\"boke\"].replace(\"�\", \"\")\n",
    "        tokenized_boke = SWE_tokenizer.encode(B[\"boke\"])\n",
    "\n",
    "        # 文章の長さが最大値を超えているか\n",
    "        if not (MIN_SENTENCE_LENGTH <= len(tokenized_boke) <= MAX_SENTENCE_LENGTH):\n",
    "            continue\n",
    "\n",
    "        tmp_bokes.append( {\n",
    "            \"boke\": boke,\n",
    "            \"star\": B[\"star\"]\n",
    "        } )\n",
    "    \n",
    "    data_dict[N] = np.random.choice(tmp_bokes, size = 1, replace = False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a7f006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"human_bokes.json\", \"w\") as f:\n",
    "    json.dump(data_dict, f, ensure_ascii = False, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20250113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
