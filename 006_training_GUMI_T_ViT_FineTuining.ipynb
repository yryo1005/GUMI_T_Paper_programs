{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb666451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 12:27:31.145209: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-06 12:27:31.153103: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-06 12:27:31.155469: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-06 12:27:31.161769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-06 12:27:31.920186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import ViTFeatureExtractor, ViTModel, PreTrainedModel, PretrainedConfig, PreTrainedTokenizerFast, AutoConfig, AutoModelForCausalLM\n",
    "from transformers.models.auto.configuration_auto import CONFIG_MAPPING\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "\n",
    "import torch   \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models.GUMI_T_ViTFC_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9274be39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/GUMI_T_ViT_FineTuining/True_True_0.75_True_31_4_16_1_8_0.0001_32_25_1024/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NO_WORD_IMAGE_ONLY = True\n",
    "REAL_IMAGE_ONLY = True\n",
    "REAL_IMAGE_THRESHOLD = 0.75\n",
    "NO_UNIQUE_NOUN_SENTENCE_ONLY = True\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "MIN_FREQUENCY = 16\n",
    "\n",
    "NUM_TRANSFORMER_LAYERS = 1 # 2, 3\n",
    "NUM_HEADS = 8 # 固定\n",
    "\n",
    "LERNING_RATE = 0.0001 # 0.001, 0.0001, 0.00001\n",
    "BATCH_SIZE = 32 # 128 # 32 # 固定\n",
    "EPOCHS = 25 # 固定\n",
    "HIDDEN_DIM = 1024 # 固定\n",
    "\n",
    "result_dir = f\"../results/GUMI_T_ViT_FineTuining/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREQUENCY}_{NUM_TRANSFORMER_LAYERS}_{NUM_HEADS}_{LERNING_RATE}_{BATCH_SIZE}_{EPOCHS}_{HIDDEN_DIM}/\"\n",
    "os.makedirs(result_dir, exist_ok= True)\n",
    "with open(f\"{result_dir}training_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"NO_WORD_IMAGE_ONLY\": NO_WORD_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_ONLY\": REAL_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_THRESHOLD\": REAL_IMAGE_THRESHOLD,\n",
    "        \"NO_UNIQUE_NOUN_SENTENCE_ONLY\": NO_UNIQUE_NOUN_SENTENCE_ONLY,\n",
    "\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MIN_FREQUENCY\": MIN_FREQUENCY,\n",
    "\n",
    "        \"NUM_TRANSFORMER_LAYERS\": NUM_TRANSFORMER_LAYERS,\n",
    "        \"NUM_HEADS\": NUM_HEADS,\n",
    "\n",
    "        \"LERNING_RATE\": LERNING_RATE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"HIDDEN_DIM\": HIDDEN_DIM\n",
    "    }, f)\n",
    "\n",
    "#\n",
    "result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953456b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1363946,),\n",
       " (1363946, 32),\n",
       " (1363946, 32),\n",
       " (14118,),\n",
       " (14118, 32),\n",
       " (14118, 32),\n",
       " 17363)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_DIR = \"../datas/Bokete_Dataset/boke_image/\"\n",
    "data_dir = f\"../datas/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREQUENCY}/\"\n",
    "\n",
    "train_inputs_1 = np.load(f\"{data_dir}train_inputs_1.npy\")\n",
    "train_inputs_2 = np.load(f\"{data_dir}train_inputs_2.npy\")\n",
    "train_teacher_signals = np.load(f\"{data_dir}train_teacher_signals.npy\")\n",
    "test_inputs_1 = np.load(f\"{data_dir}test_inputs_1.npy\")\n",
    "test_inputs_2 = np.load(f\"{data_dir}test_inputs_2.npy\")\n",
    "test_teacher_signals = np.load(f\"{data_dir}test_teacher_signals.npy\")\n",
    "tokenizer = Tokenizer.from_file(f\"{data_dir}tokenizer.json\")\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "tokenizer.save(f\"{result_dir}tokenizer.json\")\n",
    "\n",
    "#\n",
    "train_inputs_1.shape, train_inputs_2.shape, train_teacher_signals.shape, test_inputs_1.shape, test_inputs_2.shape, test_teacher_signals.shape, VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db9d034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Colab_20250113/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "image_preprocesser = ViTFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bea5523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 224, 224]), torch.Size([32, 32]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_image_dataloader(inputs_1, inputs_2, test_teacher_signals):\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, inputs_1, inputs_2, test_teacher_signals):\n",
    "            \"\"\"\n",
    "                inputs_1: 画像の番号からなるリスト\n",
    "                inputs_2: 入力文からなるリスト\n",
    "                test_teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(inputs_1) != len(inputs_2):\n",
    "                raise ValueError(\"inputs_1 and inputs_2 must have the same length.\")\n",
    "            if len(inputs_1) != len(test_teacher_signals):\n",
    "                raise ValueError(\"inputs_1 and test_teacher_signals must have the same length.\")\n",
    "\n",
    "            self.inputs_1 = inputs_1\n",
    "            self.inputs_2 = inputs_2\n",
    "            self.test_teacher_signals = test_teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs_1)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image = Image.open(f\"{IMAGE_DIR}{self.inputs_1[idx]}.jpg\").convert(\"RGB\")\n",
    "            input_sentence = torch.Tensor( self.inputs_2[idx] ).to(torch.int64)\n",
    "            teacher_signal = torch.Tensor( self.test_teacher_signals[idx] ).to(torch.int64)\n",
    "            \n",
    "            return image, input_sentence, teacher_signal\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        images, input_sentences, teacher_signals = zip(*batch)\n",
    "        \n",
    "        images = image_preprocesser(images = images, return_tensors = \"pt\")\n",
    "        input_sentences = torch.stack(input_sentences, dim = 0)\n",
    "        teacher_signals = torch.stack(teacher_signals, dim = 0)\n",
    "        \n",
    "        return images, input_sentences, teacher_signals\n",
    "\n",
    "    dataset = LoadImageDataset(inputs_1, inputs_2, test_teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        num_workers = int(os.cpu_count() * 0.6), \n",
    "        shuffle = True,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = make_image_dataloader(train_inputs_1, train_inputs_2, train_teacher_signals)\n",
    "test_dataloader = make_image_dataloader(test_inputs_1, test_inputs_2, test_teacher_signals)\n",
    "\n",
    "#\n",
    "i1, i2, t = next(iter(train_dataloader))\n",
    "i1[\"pixel_values\"].shape, i2.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parameter: 135356883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_519220/2051427529.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  generator.load_state_dict(torch.load(f\"{result_dir}generator_{START_EPOCH}.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 32001/42624 [1:16:03<25:12,  7.02it/s, loss=2.68, accuracy=0.448, perplexity=14.7]"
     ]
    }
   ],
   "source": [
    "generator = TransformerSentenceGeneratorViTFC(\n",
    "    num_transformer_layers = NUM_TRANSFORMER_LAYERS,\n",
    "    num_heads = NUM_HEADS,\n",
    "    vocab_size = len(tokenizer.get_vocab()), \n",
    "    input_length = MAX_SENTENCE_LENGTH + 1, \n",
    "    hidden_dim = HIDDEN_DIM, \n",
    "    patch_dim = 768) # pathc_dimはViTの出力次元数\n",
    "print(f\"num parameter: {sum(p.numel() for p in generator.parameters())}\")\n",
    "\n",
    "START_EPOCH = 0\n",
    "train_loss_history = list()\n",
    "test_loss_history = list()\n",
    "train_acc_history = list()\n",
    "test_acc_history = list()\n",
    "train_perplexity_history = list()\n",
    "test_perplexity_history = list()\n",
    "\n",
    "# 学習履歴がある場合，途中から再開する\n",
    "if os.path.exists(f\"{result_dir}train_history.json\"):\n",
    "    with open(f\"{result_dir}train_history.json\", \"r\") as f:\n",
    "        history = json.load(f)\n",
    "        train_loss_history = history[\"train_loss_history\"]\n",
    "        test_loss_history = history[\"test_loss_history\"]\n",
    "        train_acc_history = history[\"train_acc_history\"]\n",
    "        test_acc_history = history[\"test_acc_history\"]\n",
    "        train_perplexity_history = history[\"train_perplexity_history\"]\n",
    "        test_perplexity_history = history[\"test_perplexity_history\"]\n",
    "    \n",
    "    START_EPOCH = len(train_loss_history)\n",
    "    generator.load_state_dict(torch.load(f\"{result_dir}generator_{START_EPOCH}.pth\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = generator.to(device)\n",
    "optimizer = optim.AdamW(generator.parameters(), lr = LERNING_RATE)\n",
    "\n",
    "def calculate_accuracy(outputs, teacher_signals):\n",
    "    _, predicted = torch.max(outputs, dim=-1)\n",
    "    mask = teacher_signals != 0\n",
    "    \n",
    "    correct = (predicted == teacher_signals) & mask \n",
    "    correct = correct.sum().item()\n",
    "    \n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(loss)\n",
    "\n",
    "def train_step(generator, optimizer, inputs_1, inputs_2, teacher_signals):\n",
    "    generator.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs_1 = inputs_1.to(device)\n",
    "    inputs_2 = inputs_2.to(device)\n",
    "    teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "    outputs = generator(inputs_2, inputs_1)\n",
    "    \n",
    "    outputs = outputs.view(-1, outputs.size(-1))  # [32*32, 8192]\n",
    "    teacher_signals = teacher_signals.view(-1)    # [32*32]\n",
    "\n",
    "    loss = F.cross_entropy(outputs, teacher_signals, \n",
    "                           ignore_index = 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    accuracy = calculate_accuracy(outputs, teacher_signals)\n",
    "    perplexity = calculate_perplexity(loss)\n",
    "\n",
    "    return loss.item(), accuracy, perplexity.item()\n",
    "\n",
    "def test_step(generator, inputs_1, inputs_2, teacher_signals):\n",
    "    generator.eval()\n",
    "\n",
    "    inputs_1 = inputs_1.to(device)\n",
    "    inputs_2 = inputs_2.to(device)\n",
    "    teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = generator(inputs_2, inputs_1)\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # [32*32, 8192]\n",
    "        teacher_signals = teacher_signals.view(-1)    # [32*32]\n",
    "\n",
    "        loss = F.cross_entropy(outputs, teacher_signals, \n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(outputs, teacher_signals)\n",
    "        perplexity = calculate_perplexity(loss)\n",
    "\n",
    "    return loss.item(), accuracy, perplexity.item()\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # train\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_perplexity = 0\n",
    "    total_samples = 0\n",
    "    pb = tqdm(train_dataloader)\n",
    "    for inputs_1, inputs_2, teacher_signals in pb:\n",
    "        batch_size = inputs_2.size(0)\n",
    "\n",
    "        loss, accuracy, perplexity = train_step(generator, optimizer, inputs_1, inputs_2, teacher_signals)\n",
    "\n",
    "        train_loss += loss * batch_size\n",
    "        train_accuracy += accuracy * batch_size\n",
    "        train_perplexity += perplexity * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pb.set_postfix({\"loss\": train_loss / total_samples, \"accuracy\": train_accuracy / total_samples, \"perplexity\": train_perplexity / total_samples})\n",
    "    train_loss /= total_samples\n",
    "    train_accuracy /= total_samples\n",
    "    train_perplexity /= total_samples\n",
    "\n",
    "    # test\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    test_perplexity = 0\n",
    "    total_samples = 0\n",
    "    pb = tqdm(test_dataloader)\n",
    "    for inputs_1, inputs_2, teacher_signals in pb:\n",
    "        batch_size = inputs_2.size(0)\n",
    "\n",
    "        loss, accuracy, perplexity = test_step(generator, inputs_1, inputs_2, teacher_signals)\n",
    "\n",
    "        test_loss += loss * batch_size\n",
    "        test_accuracy += accuracy * batch_size\n",
    "        test_perplexity += perplexity * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pb.set_postfix({\"loss\": test_loss / total_samples, \"accuracy\": test_accuracy / total_samples, \"perplexity\": test_perplexity / total_samples})\n",
    "    test_loss /= total_samples\n",
    "    test_accuracy /= total_samples\n",
    "    test_perplexity /= total_samples\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test Perplexity: {test_perplexity:.4f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "    test_acc_history.append(test_accuracy)\n",
    "    train_perplexity_history.append(train_perplexity)\n",
    "    test_perplexity_history.append(test_perplexity)\n",
    "    with open(f\"{result_dir}train_history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss_history\": train_loss_history,\n",
    "            \"test_loss_history\": test_loss_history,\n",
    "            \"train_acc_history\": train_acc_history,\n",
    "            \"test_acc_history\": test_acc_history,\n",
    "            \"train_perplexity_history\": train_perplexity_history,\n",
    "            \"test_perplexity_history\": test_perplexity_history\n",
    "        }, f)\n",
    "    \n",
    "    torch.save(generator.state_dict(), f\"{result_dir}generator_{epoch + 1}.pth\")\n",
    "    if os.path.exists(f\"{result_dir}generator_{epoch}.pth\"):\n",
    "        os.remove(f\"{result_dir}generator_{epoch}.pth\")\n",
    "    \n",
    "    if min(train_loss_history) == train_loss:\n",
    "        torch.save(generator.state_dict(), f\"{result_dir}generator_best.pth\")\n",
    "        if os.path.exists(f\"{result_dir}generator_best_{epoch}.pth\"):\n",
    "            os.remove(f\"{result_dir}generator_best_{epoch}.pth\")\n",
    "\n",
    "    test_image_paths = list(sorted(set([f\"{IMAGE_DIR}{IN}.jpg\" for IN in test_inputs_1])))\n",
    "    GUMI_T_ViTFC_generate_ohgiri(image_preprocesser, generator, test_image_paths[:5], tokenizer, sentence_length = MAX_SENTENCE_LENGTH)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{result_dir}train_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUMI_T_ViTFC_Config(PretrainedConfig):\n",
    "    model_type = \"gumi_t_vitfc\"\n",
    "\n",
    "    def __init__(self, \n",
    "            num_transformer_layers = 1, \n",
    "            num_heads = 8, \n",
    "            vocab_size = 17363, \n",
    "            input_length = 32, \n",
    "            hidden_dim = 1024, \n",
    "            patch_dim = 768, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.num_transformer_layers = num_transformer_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_length = input_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.patch_dim = patch_dim\n",
    "\n",
    "class GUMI_T_ViTFC(PreTrainedModel):\n",
    "    config_class = GUMI_T_ViTFC_Config\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(GUMI_T_ViTFC, self).__init__(config)\n",
    "\n",
    "        self.input_length = config.input_length\n",
    "\n",
    "        self.image_preprocesser = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "        self.generator = TransformerSentenceGeneratorViTFC(\n",
    "            num_transformer_layers = config.num_transformer_layers,\n",
    "            num_heads = config.num_heads,\n",
    "            vocab_size = config.vocab_size, \n",
    "            input_length = config.input_length, \n",
    "            hidden_dim = config.hidden_dim, \n",
    "            patch_dim = config.patch_dim\n",
    "        )\n",
    "    \n",
    "    def load_weights(self, weight_path):\n",
    "        self.generator.load_state_dict(torch.load(weight_path))\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        os.makedirs(save_directory, exist_ok = True)\n",
    "\n",
    "        torch.save(self.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "        self.config.save_pretrained(save_directory)\n",
    "\n",
    "    def forward(self, input_ids, preprocessed_images):\n",
    "        \"\"\"\n",
    "            input_ids: 単語のIDのリスト(batch_size, input_length)\n",
    "            preprocessed_images: 画像の特徴量のリスト(batch_size, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        \n",
    "        outputs = self.generator(input_ids, preprocessed_images)\n",
    "        \n",
    "        return outputs # [batch_size, input_length, vocab_size]\n",
    "\n",
    "    def generate(self, inputs, argmax = False, k = 5, temp = 1.0):\n",
    "        device = next(self.generator.parameters()).device\n",
    "\n",
    "        pil_images = inputs[\"pixel_values\"]\n",
    "\n",
    "        preprocessed_images = self.image_preprocesser(pil_images, return_tensors = \"pt\").to(device)\n",
    "\n",
    "        gen_texts = torch.ones(size = (len(pil_images), 1)).to(torch.int32).to(device)\n",
    "\n",
    "        for i in range(0, self.input_length - 1):\n",
    "            tmp_texts = F.pad(gen_texts, (0, self.input_length - 1 - i), value = 0).to(torch.int32).to(device)\n",
    "            outputs = self.generator(tmp_texts, preprocessed_images)\n",
    "\n",
    "            logits = outputs[:, i, :]\n",
    "\n",
    "            if argmax:\n",
    "                gathered_indices = torch.argmax(logits, dim = -1, keepdim = True)\n",
    "            else:  \n",
    "                probs = F.softmax(logits / temp, dim = -1)\n",
    "                top_k_probs, top_k_indices = torch.topk(probs, k = k, dim = -1)\n",
    "                top_k_probs = top_k_probs / top_k_probs.sum(dim = -1, keepdim = True) \n",
    "                chosen_indices = torch.multinomial(top_k_probs, 1).squeeze(-1)\n",
    "                gathered_indices = top_k_indices.gather(-1, chosen_indices.unsqueeze(-1))\n",
    "\n",
    "            gen_texts = torch.cat([gen_texts, gathered_indices], dim = 1)\n",
    "        \n",
    "        return gen_texts\n",
    "    \n",
    "    def decode(self, gen_texts, tokenizer):\n",
    "        \"\"\"\n",
    "            gen_texts: 生成された文章のIDのリスト(batch_size, input_length)\n",
    "        \"\"\"\n",
    "        tmp_gen_texts = list()\n",
    "        for G in gen_texts:\n",
    "            tmp_gen_text = list()\n",
    "            for I in G[1:]:\n",
    "                if int(I) in [0, 1, 2]:\n",
    "                    break\n",
    "                tmp_gen_text.append( tokenizer.decode([int(I)]) )\n",
    "            tmp_gen_texts.append(\"\".join(tmp_gen_text))\n",
    "        \n",
    "        return tmp_gen_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d788a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_337951/794776096.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.generator.load_state_dict(torch.load(weight_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['手を挙げてもタクシーを挙げても当ててくれない人を挙げる']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file = f\"{result_dir}tokenizer.json\",)\n",
    "\n",
    "config = GUMI_T_ViTFC_Config(\n",
    "    num_transformer_layers = NUM_TRANSFORMER_LAYERS,\n",
    "    num_heads = NUM_HEADS,\n",
    "    vocab_size = len(tokenizer.get_vocab()),\n",
    "    input_length = MAX_SENTENCE_LENGTH + 1,\n",
    "    hidden_dim = HIDDEN_DIM,\n",
    "    patch_dim = 768\n",
    ")\n",
    "gumi_t = GUMI_T_ViTFC(\n",
    "    config\n",
    ")\n",
    "gumi_t.load_weights(f\"{result_dir}generator_best.pth\")\n",
    "gumi_t.to(device)\n",
    "gumi_t.eval()\n",
    "test_image_paths = [f\"test_images/{IP}\" for IP in os.listdir(\"test_images/\")]\n",
    "inputs = {\n",
    "    \"pixel_values\": [Image.open(path).convert(\"RGB\") for path in test_image_paths]\n",
    "}\n",
    "\n",
    "gen_texts = gumi_t.generate(inputs, argmax = False)\n",
    "gumi_t.decode(gen_texts, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20250113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
