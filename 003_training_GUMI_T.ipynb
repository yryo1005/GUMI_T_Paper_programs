{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb666451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import ViTFeatureExtractor, ViTModel, PreTrainedModel, PretrainedConfig, PreTrainedTokenizerFast, AutoConfig, AutoModelForCausalLM\n",
    "from transformers.models.auto.configuration_auto import CONFIG_MAPPING\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models.GUMI_T_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_WORD_IMAGE_ONLY = True\n",
    "REAL_IMAGE_ONLY = True\n",
    "REAL_IMAGE_THRESHOLD = 0.75\n",
    "NO_UNIQUE_NOUN_SENTENCE_ONLY = True\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "MIN_FREQUENCY = 16\n",
    "\n",
    "NUM_TRANSFORMER_LAYERS = 1 # 1, 2, 3, 4, 5\n",
    "NUM_HEADS = 8 # 固定\n",
    "\n",
    "LERNING_RATE = 0.0001 # 0.001, 0.0001, 0.00001\n",
    "BATCH_SIZE = 32 # 32, 128, 512\n",
    "EPOCHS = 50 # 固定\n",
    "HIDDEN_DIM = 1024 # 固定\n",
    "\n",
    "result_dir = f\"../results/GUMI_T/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREQUENCY}_{NUM_TRANSFORMER_LAYERS}_{NUM_HEADS}_{LERNING_RATE}_{BATCH_SIZE}_{EPOCHS}_{HIDDEN_DIM}/\"\n",
    "os.makedirs(result_dir, exist_ok= True)\n",
    "with open(f\"{result_dir}training_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"NO_WORD_IMAGE_ONLY\": NO_WORD_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_ONLY\": REAL_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_THRESHOLD\": REAL_IMAGE_THRESHOLD,\n",
    "        \"NO_UNIQUE_NOUN_SENTENCE_ONLY\": NO_UNIQUE_NOUN_SENTENCE_ONLY,\n",
    "\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MIN_FREQUENCY\": MIN_FREQUENCY,\n",
    "\n",
    "        \"NUM_TRANSFORMER_LAYERS\": NUM_TRANSFORMER_LAYERS,\n",
    "        \"NUM_HEADS\": NUM_HEADS,\n",
    "\n",
    "        \"LERNING_RATE\": LERNING_RATE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"HIDDEN_DIM\": HIDDEN_DIM\n",
    "    }, f)\n",
    "\n",
    "IMAGE_FEATURE_DIR = \"../datas/image_features/ViT/\"\n",
    "os.makedirs(IMAGE_FEATURE_DIR, exist_ok = True)\n",
    "\n",
    "#\n",
    "result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953456b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"../datas/Bokete_Dataset/boke_image/\"\n",
    "data_dir = f\"../datas/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREQUENCY}/\"\n",
    "\n",
    "train_inputs_1 = np.load(f\"{data_dir}train_inputs_1.npy\")\n",
    "train_inputs_2 = np.load(f\"{data_dir}train_inputs_2.npy\")\n",
    "train_teacher_signals = np.load(f\"{data_dir}train_teacher_signals.npy\")\n",
    "test_inputs_1 = np.load(f\"{data_dir}test_inputs_1.npy\")\n",
    "test_inputs_2 = np.load(f\"{data_dir}test_inputs_2.npy\")\n",
    "test_teacher_signals = np.load(f\"{data_dir}test_teacher_signals.npy\")\n",
    "tokenizer = Tokenizer.from_file(f\"{data_dir}tokenizer.json\")\n",
    "VOCAB_SIZE = tokenizer.get_vocab_size()\n",
    "\n",
    "tokenizer.save(f\"{result_dir}tokenizer.json\")\n",
    "\n",
    "#\n",
    "train_inputs_1.shape, train_inputs_2.shape, train_teacher_signals.shape, test_inputs_1.shape, test_inputs_2.shape, test_teacher_signals.shape, VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numbers = set(train_inputs_1.tolist() + test_inputs_1.tolist())\n",
    "tmp_image_numbers = list()\n",
    "for IN in image_numbers:\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp_image_numbers.append(IN)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "image_preprocesser = ViTFeatureExtractor.from_pretrained(model_name) \n",
    "vit = ViTModel.from_pretrained(model_name)\n",
    "vit = vit.to(device)\n",
    "vit.eval()\n",
    "\n",
    "bs = 512\n",
    "for idx in tqdm(range(0, len(tmp_image_numbers), bs)):\n",
    "    images = [Image.open(f\"{IMAGE_DIR}{IN}.jpg\").convert(\"RGB\") for IN in tmp_image_numbers[idx:idx + bs]]\n",
    "    preprocessed_images = image_preprocesser(images, return_tensors = \"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = vit(**preprocessed_images)\n",
    "    image_features = outputs.last_hidden_state[:, 1:, :].cpu().numpy()\n",
    "\n",
    "    for i, IN in enumerate(tmp_image_numbers[idx:idx + bs]):\n",
    "        np.save(f\"{IMAGE_FEATURE_DIR}{IN}.npy\", image_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_dataloader(inputs_1, inputs_2, test_teacher_signals):\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, inputs_1, inputs_2, test_teacher_signals):\n",
    "            \"\"\"\n",
    "                inputs_1: 画像の番号からなるリスト\n",
    "                inputs_2: 入力文からなるリスト\n",
    "                test_teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(inputs_1) != len(inputs_2):\n",
    "                raise ValueError(\"inputs_1 and inputs_2 must have the same length.\")\n",
    "            if len(inputs_1) != len(test_teacher_signals):\n",
    "                raise ValueError(\"inputs_1 and test_teacher_signals must have the same length.\")\n",
    "\n",
    "            self.inputs_1 = inputs_1\n",
    "            self.inputs_2 = inputs_2\n",
    "            self.test_teacher_signals = test_teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs_1)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = torch.Tensor( np.load(f\"{IMAGE_FEATURE_DIR}{self.inputs_1[idx]}.npy\") ).to(torch.float32)\n",
    "            input_sentence = torch.Tensor( self.inputs_2[idx] ).to(torch.int64)\n",
    "            teacher_signal = torch.Tensor( self.test_teacher_signals[idx] ).to(torch.int64)\n",
    "            \n",
    "            return image_feature, input_sentence, teacher_signal\n",
    "\n",
    "    dataset = LoadImageDataset(inputs_1, inputs_2, test_teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        num_workers = int(os.cpu_count() * 0.6), \n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = make_image_dataloader(train_inputs_1, train_inputs_2, train_teacher_signals)\n",
    "test_dataloader = make_image_dataloader(test_inputs_1, test_inputs_2, test_teacher_signals)\n",
    "\n",
    "#\n",
    "i1, i2, t = next(iter(train_dataloader))\n",
    "i1.shape, i2.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf8993",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TransformerSentenceGenerator(\n",
    "    num_transformer_layers = NUM_TRANSFORMER_LAYERS,\n",
    "    num_heads = NUM_HEADS,\n",
    "    vocab_size = len(tokenizer.get_vocab()), \n",
    "    input_length = MAX_SENTENCE_LENGTH + 1, \n",
    "    hidden_dim = HIDDEN_DIM, \n",
    "    patch_dim = 768) # pathc_dimはViTの出力次元数\n",
    "print(f\"num parameter: {sum(p.numel() for p in generator.parameters())}\")\n",
    "\n",
    "START_EPOCH = 0\n",
    "train_loss_history = list()\n",
    "test_loss_history = list()\n",
    "train_acc_history = list()\n",
    "test_acc_history = list()\n",
    "train_perplexity_history = list()\n",
    "test_perplexity_history = list()\n",
    "\n",
    "# 学習履歴がある場合，途中から再開する\n",
    "if os.path.exists(f\"{result_dir}train_history.json\"):\n",
    "    with open(f\"{result_dir}train_history.json\", \"r\") as f:\n",
    "        history = json.load(f)\n",
    "        train_loss_history = history[\"train_loss_history\"]\n",
    "        test_loss_history = history[\"test_loss_history\"]\n",
    "        train_acc_history = history[\"train_acc_history\"]\n",
    "        test_acc_history = history[\"test_acc_history\"]\n",
    "        train_perplexity_history = history[\"train_perplexity_history\"]\n",
    "        test_perplexity_history = history[\"test_perplexity_history\"]\n",
    "    \n",
    "    START_EPOCH = len(train_loss_history)\n",
    "    generator.load_state_dict(torch.load(f\"{result_dir}generator_{START_EPOCH}.pth\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = generator.to(device)\n",
    "optimizer = optim.AdamW(generator.parameters(), lr = LERNING_RATE)\n",
    "\n",
    "def calculate_accuracy(outputs, teacher_signals):\n",
    "    _, predicted = torch.max(outputs, dim=-1)\n",
    "    mask = teacher_signals != 0\n",
    "    \n",
    "    correct = (predicted == teacher_signals) & mask \n",
    "    correct = correct.sum().item()\n",
    "    \n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(loss)\n",
    "\n",
    "def train_step(generator, optimizer, inputs_1, inputs_2, teacher_signals):\n",
    "    generator.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs_1 = inputs_1.to(device)\n",
    "    inputs_2 = inputs_2.to(device)\n",
    "    teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "    outputs = generator(inputs_2, inputs_1)\n",
    "    \n",
    "    outputs = outputs.view(-1, outputs.size(-1))  # [32*32, 8192]\n",
    "    teacher_signals = teacher_signals.view(-1)    # [32*32]\n",
    "\n",
    "    loss = F.cross_entropy(outputs, teacher_signals, \n",
    "                           ignore_index = 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    accuracy = calculate_accuracy(outputs, teacher_signals)\n",
    "    perplexity = calculate_perplexity(loss)\n",
    "\n",
    "    return loss.item(), accuracy, perplexity.item()\n",
    "\n",
    "def test_step(generator, inputs_1, inputs_2, teacher_signals):\n",
    "    generator.eval()\n",
    "\n",
    "    inputs_1 = inputs_1.to(device)\n",
    "    inputs_2 = inputs_2.to(device)\n",
    "    teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = generator(inputs_2, inputs_1)\n",
    "\n",
    "        outputs = outputs.view(-1, outputs.size(-1))  # [32*32, 8192]\n",
    "        teacher_signals = teacher_signals.view(-1)    # [32*32]\n",
    "\n",
    "        loss = F.cross_entropy(outputs, teacher_signals, \n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(outputs, teacher_signals)\n",
    "        perplexity = calculate_perplexity(loss)\n",
    "\n",
    "    return loss.item(), accuracy, perplexity.item()\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # train\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_perplexity = 0\n",
    "    total_samples = 0\n",
    "    pb = tqdm(train_dataloader)\n",
    "    for inputs_1, inputs_2, teacher_signals in pb:\n",
    "        batch_size = inputs_1.size(0)\n",
    "\n",
    "        loss, accuracy, perplexity = train_step(generator, optimizer, inputs_1, inputs_2, teacher_signals)\n",
    "\n",
    "        train_loss += loss * batch_size\n",
    "        train_accuracy += accuracy * batch_size\n",
    "        train_perplexity += perplexity * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pb.set_postfix({\"loss\": train_loss / total_samples, \"accuracy\": train_accuracy / total_samples, \"perplexity\": train_perplexity / total_samples})\n",
    "    train_loss /= total_samples\n",
    "    train_accuracy /= total_samples\n",
    "    train_perplexity /= total_samples\n",
    "\n",
    "    # test\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    test_perplexity = 0\n",
    "    total_samples = 0\n",
    "    pb = tqdm(test_dataloader)\n",
    "    for inputs_1, inputs_2, teacher_signals in pb:\n",
    "        batch_size = inputs_1.size(0)\n",
    "\n",
    "        loss, accuracy, perplexity = test_step(generator, inputs_1, inputs_2, teacher_signals)\n",
    "\n",
    "        test_loss += loss * batch_size\n",
    "        test_accuracy += accuracy * batch_size\n",
    "        test_perplexity += perplexity * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pb.set_postfix({\"loss\": test_loss / total_samples, \"accuracy\": test_accuracy / total_samples, \"perplexity\": test_perplexity / total_samples})\n",
    "    test_loss /= total_samples\n",
    "    test_accuracy /= total_samples\n",
    "    test_perplexity /= total_samples\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test Perplexity: {test_perplexity:.4f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "    test_acc_history.append(test_accuracy)\n",
    "    train_perplexity_history.append(train_perplexity)\n",
    "    test_perplexity_history.append(test_perplexity)\n",
    "    with open(f\"{result_dir}train_history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss_history\": train_loss_history,\n",
    "            \"test_loss_history\": test_loss_history,\n",
    "            \"train_acc_history\": train_acc_history,\n",
    "            \"test_acc_history\": test_acc_history,\n",
    "            \"train_perplexity_history\": train_perplexity_history,\n",
    "            \"test_perplexity_history\": test_perplexity_history\n",
    "        }, f)\n",
    "    \n",
    "    torch.save(generator.state_dict(), f\"{result_dir}generator_{epoch + 1}.pth\")\n",
    "    if os.path.exists(f\"{result_dir}generator_{epoch}.pth\") and epoch % 5 != 0:\n",
    "        os.remove(f\"{result_dir}generator_{epoch}.pth\")\n",
    "    \n",
    "    if min(train_loss_history) == train_loss:\n",
    "        torch.save(generator.state_dict(), f\"{result_dir}generator_best.pth\")\n",
    "        if os.path.exists(f\"{result_dir}generator_best_{epoch}.pth\"):\n",
    "            os.remove(f\"{result_dir}generator_best_{epoch}.pth\")\n",
    "\n",
    "    test_image_paths = list(sorted(set([f\"{IMAGE_DIR}{IN}.jpg\" for IN in test_inputs_1])))\n",
    "    GUMI_T_generate_ohgiri(vit, image_preprocesser, generator, test_image_paths[:5], tokenizer, sentence_length = MAX_SENTENCE_LENGTH)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{result_dir}train_history.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd09f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file = f\"{result_dir}tokenizer.json\",)\n",
    "\n",
    "config = GUMI_T_Config(\n",
    "    num_transformer_layers = NUM_TRANSFORMER_LAYERS,\n",
    "    num_heads = NUM_HEADS,\n",
    "    vocab_size = len(tokenizer.get_vocab()),\n",
    "    input_length = MAX_SENTENCE_LENGTH + 1,\n",
    "    hidden_dim = HIDDEN_DIM,\n",
    "    patch_dim = 768\n",
    ")\n",
    "gumi_t = GUMI_T(\n",
    "    config\n",
    ")\n",
    "gumi_t.load_weights(f\"{result_dir}generator_best.pth\")\n",
    "gumi_t.to(device)\n",
    "gumi_t.eval()\n",
    "test_image_paths = [f\"test_images/{IP}\" for IP in os.listdir(\"test_images/\")]\n",
    "inputs = {\n",
    "    \"pixel_values\": [Image.open(path).convert(\"RGB\") for path in test_image_paths]\n",
    "}\n",
    "\n",
    "gen_texts = gumi_t.generate(inputs, argmax = False)\n",
    "gumi_t.decode(gen_texts, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20250113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
