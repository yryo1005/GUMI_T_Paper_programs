{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb666451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_WORD_IMAGE_ONLY = True\n",
    "REAL_IMAGE_ONLY = True\n",
    "REAL_IMAGE_THRESHOLD = 0.75\n",
    "NO_UNIQUE_NOUN_SENTENCE_ONLY = True\n",
    "\n",
    "VOCAB_SIZE = 8192\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "MIN_FREEQENCY = 16\n",
    "\n",
    "LERNING_RATE = 0.0001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "result_dir = f\"../results/GUMI_T/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{VOCAB_SIZE}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREEQENCY}_{LERNING_RATE}_{BATCH_SIZE}_{EPOCHS}_{HIDDEN_DIM}/\"\n",
    "if os.path.exists(result_dir):\n",
    "    shutil.rmtree(result_dir)\n",
    "os.makedirs(result_dir)\n",
    "with open(f\"{result_dir}training_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"NO_WORD_IMAGE_ONLY\": NO_WORD_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_ONLY\": REAL_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_THRESHOLD\": REAL_IMAGE_THRESHOLD,\n",
    "        \"NO_UNIQUE_NOUN_SENTENCE_ONLY\": NO_UNIQUE_NOUN_SENTENCE_ONLY,\n",
    "\n",
    "        \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MIN_FREEQENCY\": MIN_FREEQENCY,\n",
    "\n",
    "        \"LERNING_RATE\": LERNING_RATE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"HIDDEN_DIM\": HIDDEN_DIM\n",
    "    }, f)\n",
    "\n",
    "IMAGE_FEATURE_DIR = \"../data/image_features/ViT/\"\n",
    "os.makedirs(IMAGE_FEATURE_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953456b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1388020,),\n",
       " (1388020, 32),\n",
       " (1388020, 32),\n",
       " (13791,),\n",
       " (13791, 32),\n",
       " (13791, 32),\n",
       " 8192)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_DIR = \"../datas/Bokete_Dataset/boke_image/\"\n",
    "data_dir = f\"../datas/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{VOCAB_SIZE}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREEQENCY}/\"\n",
    "\n",
    "train_inputs_1 = np.load(f\"{data_dir}train_inputs_1.npy\")\n",
    "train_inputs_2 = np.load(f\"{data_dir}train_inputs_2.npy\")\n",
    "train_teacher_signals = np.load(f\"{data_dir}train_teacher_signals.npy\")\n",
    "test_inputs_1 = np.load(f\"{data_dir}test_inputs_1.npy\")\n",
    "test_inputs_2 = np.load(f\"{data_dir}test_inputs_2.npy\")\n",
    "test_teacher_signals = np.load(f\"{data_dir}test_teacher_signals.npy\")\n",
    "tokenizer = Tokenizer.from_file(f\"{data_dir}tokenizer.json\")\n",
    "\n",
    "#\n",
    "train_inputs_1.shape, train_inputs_2.shape, train_teacher_signals.shape, test_inputs_1.shape, test_inputs_2.shape, test_teacher_signals.shape, len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Colab_20250113/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "image_numbers = set(train_inputs_1.tolist() + test_inputs_1.tolist())\n",
    "tmp_image_numbers = list()\n",
    "for IN in image_numbers:\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp_image_numbers.append(IN)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "image_feature_extractor = ViTFeatureExtractor.from_pretrained(model_name) \n",
    "vit = ViTModel.from_pretrained(model_name)\n",
    "vit = vit.to(device)\n",
    "vit.eval()\n",
    "\n",
    "bs = 512\n",
    "for idx in tqdm(range(0, len(tmp_image_numbers), bs)):\n",
    "    images = [Image.open(f\"{IMAGE_DIR}{IN}.jpg\").convert(\"RGB\") for IN in tmp_image_numbers[idx:idx + bs]]\n",
    "    images = image_feature_extractor(images, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = vit(**images)\n",
    "    image_features = outputs.last_hidden_state[:, 1:, :].cpu().numpy()\n",
    "\n",
    "    for i, IN in enumerate(tmp_image_numbers[idx:idx + bs]):\n",
    "        np.save(f\"{IMAGE_FEATURE_DIR}{IN}.npy\", image_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bea5523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 196, 768]), torch.Size([32, 32]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_image_dataloader(inputs_1, inputs_2, test_teacher_signals):\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, inputs_1, inputs_2, test_teacher_signals):\n",
    "            \"\"\"\n",
    "                inputs_1: 画像の番号からなるリスト\n",
    "                inputs_2: 入力文からなるリスト\n",
    "                test_teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(inputs_1) != len(inputs_2):\n",
    "                raise ValueError(\"inputs_1 and inputs_2 must have the same length.\")\n",
    "            if len(inputs_1) != len(test_teacher_signals):\n",
    "                raise ValueError(\"inputs_1 and test_teacher_signals must have the same length.\")\n",
    "\n",
    "            self.inputs_1 = inputs_1\n",
    "            self.inputs_2 = inputs_2\n",
    "            self.test_teacher_signals = test_teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs_1)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = torch.Tensor( np.load(f\"{IMAGE_FEATURE_DIR}{self.inputs_1[idx]}.npy\") ).to(torch.float32)\n",
    "            input_sentence = torch.Tensor( self.inputs_2[idx] ).to(torch.int64)\n",
    "            teacher_signal = torch.Tensor( self.test_teacher_signals[idx] ).to(torch.int64)\n",
    "            \n",
    "            return image_feature, input_sentence, teacher_signal\n",
    "\n",
    "    dataset = LoadImageDataset(inputs_1, inputs_2, test_teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        num_workers = int(os.cpu_count() * 0.6), \n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = make_image_dataloader(train_inputs_1, train_inputs_2, train_teacher_signals)\n",
    "test_dataloader = make_image_dataloader(test_inputs_1, test_inputs_2, test_teacher_signals)\n",
    "\n",
    "#\n",
    "i1, i2, t = next(iter(train_dataloader))\n",
    "i1.shape, i2.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ffa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, input_length):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_length = input_length\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim = hidden_dim, \n",
    "            num_heads = 8, \n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, _, _ = x.size()\n",
    "        attn_mask = torch.triu(torch.ones(self.input_length, self.input_length), diagonal = 1).bool()\n",
    "        attn_mask = attn_mask.to(x.device)\n",
    "        # attn_mask = attn_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        attn_output, attn_output_weights  = self.self_attention(x, x, x, attn_mask = attn_mask)\n",
    "        x = self.layer_norm( x + attn_output )\n",
    "        x = nn.LeakyReLU()( self.fc(x) )\n",
    "        return x, attn_output_weights\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, input_length):\n",
    "        super(CrossAttentionBlock, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_length = input_length\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim = hidden_dim, \n",
    "            num_heads = 8, \n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        attn_output, attn_output_weights  = self.cross_attention(x, y, y)\n",
    "        x = self.layer_norm( x + attn_output )\n",
    "        x = nn.LeakyReLU()( self.fc(x) )\n",
    "        return x, attn_output_weights\n",
    "\n",
    "class SentenceGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, input_length, hidden_dim, patch_dim, ):\n",
    "        super(SentenceGenerator, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(patch_dim, hidden_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.sa_1 = SelfAttentionBlock(vocab_size, hidden_dim, input_length)\n",
    "        self.ca_1 = CrossAttentionBlock(vocab_size, hidden_dim, input_length)\n",
    "        self.sa_2 = SelfAttentionBlock(vocab_size, hidden_dim, input_length)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "            x: 文章\n",
    "            y: 画像の特徴量\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        y = self.fc_1(y)\n",
    "        \n",
    "        x, sa_1_weights = self.sa_1(x)\n",
    "        x, ca_1_weights = self.ca_1(x, y)\n",
    "        x, sa_2_weights = self.sa_2(x)\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "        return x, sa_1_weights, ca_1_weights, sa_2_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadcff8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1037063/571656559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mtest_image_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{IMAGE_DIR}{IN}.jpg\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mIN\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_inputs_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mgenerate_ohgiri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_feature_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_ohgiri(vit, image_feature_extractor, generator, image_paths, tokenizer):\n",
    "    \"\"\"\n",
    "        image_paths: 画像のパスのリスト\n",
    "    \"\"\"\n",
    "\n",
    "    device = next(generator.parameters()).device\n",
    "\n",
    "    images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
    "    tmp_images = image_feature_extractor(images, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = vit(**tmp_images)\n",
    "    image_features = outputs.last_hidden_state[:, 1:, :]\n",
    "\n",
    "    gen_texts = torch.ones(size = (len(image_paths), 1)).to(torch.int32).to(device)\n",
    "\n",
    "    for i in range(1, MAX_SENTENCE_LENGTH + 1):\n",
    "        tmp_texts = F.pad(gen_texts, (0, MAX_SENTENCE_LENGTH + 1 - i), value = tokenizer.token_to_id(\"[pad]\")).to(torch.int32).to(device)\n",
    "        output, _, _, _ = generator(tmp_texts, image_features)\n",
    "\n",
    "        logits = output[:, i, :]\n",
    "        probs = F.softmax(logits, dim = -1)\n",
    "\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, k = 5, dim = -1)\n",
    "        top_k_probs = top_k_probs / top_k_probs.sum(dim = -1, keepdim = True) \n",
    "        chosen_indices = torch.multinomial(top_k_probs, 1).squeeze(-1)\n",
    "        gathered_indices = top_k_indices.gather(-1, chosen_indices.unsqueeze(-1))\n",
    "\n",
    "        gen_texts = torch.cat([gen_texts, gathered_indices], dim = 1)\n",
    "    \n",
    "    fig = plt.figure(figsize = (20, 20))\n",
    "    for i, (I, IP) in enumerate(zip(images, image_paths)):\n",
    "        IP = IP.split(\"/\")[-1]\n",
    "        ax = fig.add_subplot(1, len(image_paths), i + 1)\n",
    "        ax.imshow(I)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"{i}: {IP}\")\n",
    "    plt.show()\n",
    "    \n",
    "    for i, G in enumerate(gen_texts):\n",
    "        genetated_sentence = tokenizer.decode(G.tolist())\n",
    "        genetated_sentence = genetated_sentence.replace(\"[PAD]\", \"\").replace(\"[BOS]\", \"\").replace(\"[EOS]\", \"\").replace(\" \", \"\")\n",
    "        print(f\"{i}: {genetated_sentence}\")\n",
    "\n",
    "# test_image_paths = list(set([f\"{IMAGE_DIR}{IN}.jpg\" for IN in test_inputs_1]))\n",
    "# generate_ohgiri(vit, image_feature_extractor, generator, test_image_paths[:5], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf8993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 9079/43376 [08:31<32:32, 17.56it/s, loss=5.97, accuracy=0.153, perplexity=458]      "
     ]
    }
   ],
   "source": [
    "generator = SentenceGenerator(len(tokenizer.get_vocab()), MAX_SENTENCE_LENGTH + 1, HIDDEN_DIM, 768)\n",
    "\n",
    "START_EPOCH = 0\n",
    "train_loss_history = list()\n",
    "test_loss_history = list()\n",
    "train_acc_history = list()\n",
    "test_acc_history = list()\n",
    "train_perplexity_history = list()\n",
    "test_perplexity_history = list()\n",
    "\n",
    "# 学習履歴がある場合，途中から再開する\n",
    "if os.path.exists(f\"{result_dir}train_history.json\"):\n",
    "    with open(f\"{result_dir}train_history.json\", \"r\") as f:\n",
    "        history = json.load(f)\n",
    "        train_loss_history = history[\"train_loss_history\"]\n",
    "        test_loss_history = history[\"test_loss_history\"]\n",
    "        train_acc_history = history[\"train_acc_history\"]\n",
    "        test_acc_history = history[\"test_acc_history\"]\n",
    "    \n",
    "    START_EPOCH = len(train_loss_history)\n",
    "    generator.load_state_dict(torch.load(f\"{result_dir}generator_{START_EPOCH}.pth\"))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "generator = generator.to(device)\n",
    "optimizer = optim.Adam(generator.parameters(), lr = LERNING_RATE)\n",
    "\n",
    "def calculate_accuracy(outputs, teacher_signals):\n",
    "    _, predicted = torch.max(outputs, dim=-1)\n",
    "    mask = teacher_signals != 0\n",
    "    \n",
    "    correct = (predicted == teacher_signals) & mask \n",
    "    correct = correct.sum().item()\n",
    "    \n",
    "    total = mask.sum().item()\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "def calculate_perplexity(loss):\n",
    "    return torch.exp(loss)\n",
    "\n",
    "def train_step(generator, optimizer, inputs_1, inputs_2, teacher_signals):\n",
    "    generator.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs_1 = inputs_1.to(device)\n",
    "    inputs_2 = inputs_2.to(device)\n",
    "    teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "    outputs, _, _, _ = generator(inputs_2, inputs_1)\n",
    "    \n",
    "    outputs = outputs.view(-1, outputs.size(-1))  # [32*32, 8192]\n",
    "    teacher_signals = teacher_signals.view(-1)    # [32*32]\n",
    "\n",
    "    loss = F.cross_entropy(outputs, teacher_signals, \n",
    "                           ignore_index = 0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    accuracy = calculate_accuracy(outputs, teacher_signals)\n",
    "    perplexity = calculate_perplexity(loss)\n",
    "\n",
    "    return loss.item(), accuracy, perplexity.item()\n",
    "\n",
    "def test_step(generator, inputs_1, inputs_2, teacher_signals):\n",
    "    generator.eval()\n",
    "\n",
    "    inputs_1 = inputs_1.to(device)\n",
    "    inputs_2 = inputs_2.to(device)\n",
    "    teacher_signals = teacher_signals.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs, _, _, _ = generator(inputs_2, inputs_1)\n",
    "\n",
    "        loss = F.cross_entropy(outputs, teacher_signals, \n",
    "                               ignore_index = 0)\n",
    "        accuracy = calculate_accuracy(outputs, teacher_signals)\n",
    "        perplexity = calculate_perplexity(loss)\n",
    "\n",
    "    return loss.item(), accuracy, perplexity.item()\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # train\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    train_perplexity = 0\n",
    "    total_samples = 0\n",
    "    pb = tqdm(train_dataloader)\n",
    "    for inputs_1, inputs_2, teacher_signals in pb:\n",
    "        batch_size = inputs_1.size(0)\n",
    "\n",
    "        loss, accuracy, perplexity = train_step(generator, optimizer, inputs_1, inputs_2, teacher_signals)\n",
    "\n",
    "        train_loss += loss * batch_size\n",
    "        train_accuracy += accuracy * batch_size\n",
    "        train_perplexity += perplexity * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pb.set_postfix({\"loss\": train_loss / total_samples, \"accuracy\": train_accuracy / total_samples, \"perplexity\": train_perplexity / total_samples})\n",
    "    train_loss /= total_samples\n",
    "    train_accuracy /= total_samples\n",
    "    train_perplexity /= total_samples\n",
    "\n",
    "    # test\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    test_perplexity = 0\n",
    "    total_samples = 0\n",
    "    pb = tqdm(test_dataloader)\n",
    "    for inputs_1, inputs_2, teacher_signals in pb:\n",
    "        batch_size = inputs_1.size(0)\n",
    "\n",
    "        loss, accuracy, perplexity = test_step(generator, inputs_1, inputs_2, teacher_signals)\n",
    "\n",
    "        test_loss += loss * batch_size\n",
    "        test_accuracy += accuracy * batch_size\n",
    "        test_perplexity += perplexity * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "        pb.set_postfix({\"loss\": test_loss / total_samples, \"accuracy\": test_accuracy / total_samples, \"perplexity\": test_perplexity / total_samples})\n",
    "    test_loss /= total_samples\n",
    "    test_accuracy /= total_samples\n",
    "    test_perplexity /= total_samples\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Perplexity: {train_perplexity:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test Perplexity: {test_perplexity:.4f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    test_loss_history.append(test_loss)\n",
    "    train_acc_history.append(train_accuracy)\n",
    "    test_acc_history.append(test_accuracy)\n",
    "    train_perplexity_history.append(train_perplexity)\n",
    "    test_perplexity_history.append(test_perplexity)\n",
    "    with open(f\"{result_dir}train_history.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"train_loss_history\": train_loss_history,\n",
    "            \"test_loss_history\": test_loss_history,\n",
    "            \"train_acc_history\": train_acc_history,\n",
    "            \"test_acc_history\": test_acc_history,\n",
    "            \"train_perplexity_history\": train_perplexity_history,\n",
    "            \"test_perplexity_history\": test_perplexity_history\n",
    "        }, f)\n",
    "    \n",
    "    torch.save(generator.state_dict(), f\"{result_dir}generator_{epoch + 1}.pth\")\n",
    "    if os.path.exists(f\"{result_dir}generator_{epoch}.pth\"):\n",
    "        os.remove(f\"{result_dir}generator_{epoch}.pth\")\n",
    "    \n",
    "    if min(train_loss_history) == train_loss:\n",
    "        torch.save(generator.state_dict(), f\"{result_dir}generator_best.pth\")\n",
    "        if os.path.exists(f\"{result_dir}generator_best_{epoch}.pth\"):\n",
    "            os.remove(f\"{result_dir}generator_best_{epoch}.pth\")\n",
    "\n",
    "    test_image_paths = list(set([f\"{IMAGE_DIR}{IN}.jpg\" for IN in test_inputs_1]))\n",
    "    generate_ohgiri(vit, image_feature_extractor, generator, test_image_paths[:5], tokenizer)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# 学習結果を描画\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_loss_history, label = \"train\")\n",
    "ax.plot(test_loss_history, label = \"test\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(f\"{result_dir}train_history.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20250113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
