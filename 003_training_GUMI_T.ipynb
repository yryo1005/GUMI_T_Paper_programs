{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb666451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 18:46:22.742389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-27 18:46:22.750729: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-27 18:46:22.753174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-27 18:46:22.759390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 18:46:23.255947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9274be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_WORD_IMAGE_ONLY = True\n",
    "REAL_IMAGE_ONLY = True\n",
    "REAL_IMAGE_THRESHOLD = 0.75\n",
    "NO_UNIQUE_NOUN_SENTENCE_ONLY = True\n",
    "\n",
    "VOCAB_SIZE = 8192\n",
    "MAX_SENTENCE_LENGTH = 31\n",
    "MIN_SENTENCE_LENGTH = 4\n",
    "MIN_FREEQENCY = 16\n",
    "\n",
    "LERNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 25\n",
    "HIDDEN_DIM = 512\n",
    "\n",
    "result_dir = f\"../results/GUMI_T/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{VOCAB_SIZE}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREEQENCY}_{LERNING_RATE}_{BATCH_SIZE}_{EPOCHS}_{HIDDEN_DIM}/\"\n",
    "if os.path.exists(result_dir):\n",
    "    shutil.rmtree(result_dir)\n",
    "os.makedirs(result_dir)\n",
    "with open(f\"{result_dir}training_config.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"NO_WORD_IMAGE_ONLY\": NO_WORD_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_ONLY\": REAL_IMAGE_ONLY,\n",
    "        \"REAL_IMAGE_THRESHOLD\": REAL_IMAGE_THRESHOLD,\n",
    "        \"NO_UNIQUE_NOUN_SENTENCE_ONLY\": NO_UNIQUE_NOUN_SENTENCE_ONLY,\n",
    "\n",
    "        \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "        \"MAX_SENTENCE_LENGTH\": MAX_SENTENCE_LENGTH,\n",
    "        \"MIN_SENTENCE_LENGTH\": MIN_SENTENCE_LENGTH,\n",
    "        \"MIN_FREEQENCY\": MIN_FREEQENCY,\n",
    "\n",
    "        \"LERNING_RATE\": LERNING_RATE,\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"EPOCHS\": EPOCHS,\n",
    "        \"HIDDEN_DIM\": HIDDEN_DIM\n",
    "    }, f)\n",
    "\n",
    "IMAGE_FEATURE_DIR = \"../data/image_features/ViT/\"\n",
    "os.makedirs(IMAGE_FEATURE_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953456b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1388020,), (1388020, 32), (1388020, 32), (13791,), (13791, 32), (13791, 32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMAGE_DIR = \"../datas/Bokete_Dataset/boke_image/\"\n",
    "data_dir = f\"../datas/{NO_WORD_IMAGE_ONLY}_{REAL_IMAGE_ONLY}_{REAL_IMAGE_THRESHOLD}_{NO_UNIQUE_NOUN_SENTENCE_ONLY}_{VOCAB_SIZE}_{MAX_SENTENCE_LENGTH}_{MIN_SENTENCE_LENGTH}_{MIN_FREEQENCY}/\"\n",
    "\n",
    "train_inputs_1 = np.load(f\"{data_dir}train_inputs_1.npy\")\n",
    "train_inputs_2 = np.load(f\"{data_dir}train_inputs_2.npy\")\n",
    "train_teacher_signals = np.load(f\"{data_dir}train_teacher_signals.npy\")\n",
    "test_inputs_1 = np.load(f\"{data_dir}test_inputs_1.npy\")\n",
    "test_inputs_2 = np.load(f\"{data_dir}test_inputs_2.npy\")\n",
    "test_teacher_signals = np.load(f\"{data_dir}test_teacher_signals.npy\")\n",
    "\n",
    "#\n",
    "train_inputs_1.shape, train_inputs_2.shape, train_teacher_signals.shape, test_inputs_1.shape, test_inputs_2.shape, test_teacher_signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/Colab_20250113/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/432 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "image_numbers = set(train_inputs_1.tolist() + test_inputs_1.tolist())\n",
    "tmp_image_numbers = list()\n",
    "for IN in image_numbers:\n",
    "    if os.path.exists(f\"{IMAGE_FEATURE_DIR}{IN}.npy\"):\n",
    "        continue\n",
    "    tmp_image_numbers.append(IN)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
    "vit = ViTModel.from_pretrained(model_name)\n",
    "vit = vit.to(device)\n",
    "vit.eval()\n",
    "\n",
    "bs = 512\n",
    "for idx in tqdm(range(0, len(tmp_image_numbers), bs)):\n",
    "    images = [Image.open(f\"{IMAGE_DIR}{IN}.jpg\").convert(\"RGB\") for IN in tmp_image_numbers[idx:idx + bs]]\n",
    "    images = feature_extractor(images, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = vit(**images)\n",
    "    image_features = outputs.last_hidden_state[:, 1:, :].cpu().numpy()\n",
    "\n",
    "    for i, IN in enumerate(tmp_image_numbers[idx:idx + bs]):\n",
    "        np.save(f\"{IMAGE_FEATURE_DIR}{IN}.npy\", image_features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90862a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 196, 768)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bea5523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 768]), torch.Size([32, 32]), torch.Size([32, 32]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_image_dataloader(inputs_1, inputs_2, test_teacher_signals):\n",
    "    class LoadImageDataset(Dataset):\n",
    "        def __init__(self, inputs_1, inputs_2, test_teacher_signals):\n",
    "            \"\"\"\n",
    "                inputs_1: 画像の番号からなるリスト\n",
    "                inputs_2: 入力文からなるリスト\n",
    "                test_teacher_signals: 教師信号からなるリスト\n",
    "            \"\"\"\n",
    "            if len(inputs_1) != len(inputs_2):\n",
    "                raise ValueError(\"inputs_1 and inputs_2 must have the same length.\")\n",
    "            if len(inputs_1) != len(test_teacher_signals):\n",
    "                raise ValueError(\"inputs_1 and test_teacher_signals must have the same length.\")\n",
    "\n",
    "            self.inputs_1 = inputs_1\n",
    "            self.inputs_2 = inputs_2\n",
    "            self.test_teacher_signals = test_teacher_signals\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.inputs_1)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            image_feature = torch.Tensor( np.load(f\"{IMAGE_FEATURE_DIR}{self.inputs_1[idx]}.npy\") ).to(torch.float32)\n",
    "            input_sentence = torch.Tensor( self.inputs_2[idx] ).to(torch.int32)\n",
    "            teacher_signal = torch.Tensor( self.test_teacher_signals[idx] ).to(torch.int32)\n",
    "            \n",
    "            return image_feature, input_sentence, teacher_signal\n",
    "\n",
    "    dataset = LoadImageDataset(inputs_1, inputs_2, test_teacher_signals)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        num_workers = int(os.cpu_count() * 0.6), \n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "train_dataloader = make_image_dataloader(train_inputs_1, train_inputs_2, train_teacher_signals)\n",
    "test_dataloader = make_image_dataloader(test_inputs_1, test_inputs_2, test_teacher_signals)\n",
    "\n",
    "#\n",
    "i1, i2, t = next(iter(train_dataloader))\n",
    "i1.shape, i2.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff2ffa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, input_length):\n",
    "        super(SelfAttentionBlock, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_length = input_length\n",
    "\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim = hidden_dim, \n",
    "            num_heads = 8, \n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size, _, _ = x.size()\n",
    "        attn_mask = torch.triu(torch.ones(self.input_length, self.input_length), diagonal = 1).bool()\n",
    "        attn_mask = attn_mask.to(x.device)\n",
    "        # attn_mask = attn_mask.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "        attn_output, attn_output_weights  = self.self_attention(x, x, x, attn_mask = attn_mask)\n",
    "        x = self.layer_norm( x + attn_output )\n",
    "        x = nn.LeakyReLU()( self.fc(x) )\n",
    "        return x, attn_output_weights\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, input_length):\n",
    "        super(CrossAttentionBlock, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_length = input_length\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim = hidden_dim, \n",
    "            num_heads = 8, \n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "\n",
    "\n",
    "\n",
    "        attn_output, attn_output_weights  = self.cross_attention(x, y, y)\n",
    "        x = self.layer_norm( x + attn_output )\n",
    "        x = nn.LeakyReLU()( self.fc(x) )\n",
    "        return x, attn_output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee174242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, input_length):\n",
    "        super(SentenceDecoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.sa_1 = SelfAttentionBlock(vocab_size, hidden_dim, input_length)\n",
    "        self.ca_1 = CrossAttentionBlock(vocab_size, hidden_dim, input_length)\n",
    "        self.sa_2 = SelfAttentionBlock(vocab_size, hidden_dim, input_length)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\n",
    "            x: 文章\n",
    "            y: 画像の特徴量\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x, sa_1_weights = self.sa_1(x)\n",
    "        print(x.shape, y.shape)\n",
    "        x, ca_1_weights = self.ca_1(x, y)\n",
    "        x, sa_2_weights = self.sa_2(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x, sa_1_weights, ca_1_weights, sa_2_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "260c68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 8192]) torch.Size([32, 768])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "For batched (3-D) `query`, expected `key` and `value` to be 3-D but found 2-D and 2-D tensors respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1006855/3818444950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1006855/1060035030.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_1_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_1_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa_2_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1006855/2826061072.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             )\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1369\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6012\u001b[0m         )\n\u001b[1;32m   6013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6014\u001b[0;31m     is_batched = _mha_shape_check(\n\u001b[0m\u001b[1;32m   6015\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m     )\n",
      "\u001b[0;32m/home/user/Colab_20250113/lib/python3.11/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_mha_shape_check\u001b[0;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001b[0m\n\u001b[1;32m   5784\u001b[0m         \u001b[0;31m# Batched Inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5785\u001b[0m         \u001b[0mis_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5786\u001b[0;31m         assert key.dim() == 3 and value.dim() == 3, (\n\u001b[0m\u001b[1;32m   5787\u001b[0m             \u001b[0;34m\"For batched (3-D) `query`, expected `key` and `value` to be 3-D\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5788\u001b[0m             \u001b[0;34mf\" but found {key.dim()}-D and {value.dim()}-D tensors respectively\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: For batched (3-D) `query`, expected `key` and `value` to be 3-D but found 2-D and 2-D tensors respectively"
     ]
    }
   ],
   "source": [
    "model = SentenceDecoder(VOCAB_SIZE, HIDDEN_DIM, MAX_SENTENCE_LENGTH + 1)\n",
    "model = model.to(device)\n",
    "\n",
    "model(i2.to(device), i1.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e805886d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1283, -0.0771, -0.1870,  ...,  0.0586, -0.0634, -0.0775],\n",
       "        [-0.0104, -0.2622,  0.3650,  ..., -0.3836, -0.2408, -0.0771],\n",
       "        [-0.0657, -0.1895, -0.0658,  ..., -0.1393,  0.0847, -0.0212],\n",
       "        ...,\n",
       "        [-0.1642, -0.1845, -0.0161,  ...,  0.1604, -0.0831,  0.1903],\n",
       "        [ 0.0532, -0.1612, -0.1201,  ...,  0.1068, -0.0672,  0.1242],\n",
       "        [ 0.1493, -0.1318, -0.1077,  ...,  0.0862, -0.4769, -0.1203]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85254178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 3353, 1886,  ...,    0,    0,    0],\n",
       "        [   1,  116,  111,  ...,    0,    0,    0],\n",
       "        [   1, 2783, 2772,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   1, 4264, 6637,  ...,    0,    0,    0],\n",
       "        [   1, 2562,   49,  ...,    0,    0,    0],\n",
       "        [   1, 5498, 2531,  ...,    0,    0,    0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Colab_20250113",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
